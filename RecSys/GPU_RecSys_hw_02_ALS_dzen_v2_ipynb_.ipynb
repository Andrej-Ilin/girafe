{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aaf10263",
      "metadata": {
        "id": "aaf10263"
      },
      "source": [
        "# ALS applications"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7468264",
      "metadata": {
        "id": "b7468264"
      },
      "source": [
        "## Dzen dataset\n",
        "\n",
        "Data comes from [dzen.ru](https://dzen.ru/) site and consists of likes which users put to text articles\n",
        "\n",
        "### Columns\n",
        "1. item_id - unique id of an item (article)\n",
        "2. user_id - unique id of a user\n",
        "3. source_id - unique id of an author. If two items have same source_id, then they come from one author\n",
        "4. Name of item is name of the article\n",
        "5. Raw dataset represents user_id and list of item_ids which user liked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "564cb47b-ba99-40d2-a0c4-9987cefe1253",
      "metadata": {
        "id": "564cb47b-ba99-40d2-a0c4-9987cefe1253",
        "outputId": "f606a418-ae87-4b23-9c4b-ed10ae0596b8"
      },
      "outputs": [],
      "source": [
        "# !curl -O -J -L 'https://www.dropbox.com/s/ia4bvhuqg8kesee/zen_dataset.zip?dl=1'\n",
        "# !unzip zen_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "51630f5f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-07T21:31:43.942096Z",
          "start_time": "2023-06-07T21:31:43.139524Z"
        },
        "execution": {
          "iopub.execute_input": "2024-03-21T10:51:45.412325Z",
          "iopub.status.busy": "2024-03-21T10:51:45.411479Z",
          "iopub.status.idle": "2024-03-21T10:51:45.790251Z",
          "shell.execute_reply": "2024-03-21T10:51:45.789486Z",
          "shell.execute_reply.started": "2024-03-21T10:51:45.412266Z"
        },
        "id": "51630f5f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "from tqdm.notebook import tqdm\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "90965805",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-07T21:31:45.414950Z",
          "start_time": "2023-06-07T21:31:44.581759Z"
        },
        "execution": {
          "iopub.execute_input": "2024-03-21T10:21:38.347864Z",
          "iopub.status.busy": "2024-03-21T10:21:38.346423Z",
          "iopub.status.idle": "2024-03-21T10:21:49.584167Z",
          "shell.execute_reply": "2024-03-21T10:21:49.583861Z",
          "shell.execute_reply.started": "2024-03-21T10:21:38.347806Z"
        },
        "id": "90965805"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "item_names = pd.read_csv(\"zen_item_to_name.csv\")\n",
        "item_sources = pd.read_csv(\"zen_item_to_source.csv\")\n",
        "dataset = pd.read_csv(\"zen_ratings.csv\", converters={'item_ids': ast.literal_eval})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e8820e36",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-07T21:31:55.339206Z",
          "start_time": "2023-06-07T21:31:51.973369Z"
        },
        "colab": {
          "referenced_widgets": [
            "8a5a8ae4cb8b4ea4911a57ce4a09dbb9"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-03-21T10:24:41.880387Z",
          "iopub.status.busy": "2024-03-21T10:24:41.879749Z",
          "iopub.status.idle": "2024-03-21T10:24:45.217848Z",
          "shell.execute_reply": "2024-03-21T10:24:45.217314Z",
          "shell.execute_reply.started": "2024-03-21T10:24:41.880355Z"
        },
        "id": "e8820e36",
        "outputId": "1d425f2c-550a-4dfc-95f7-615e5b4e47c5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17358cbaaef845a3ba8f3c3cb8705199",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/75910 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Total number of interactions\n",
        "total_interactions_count = dataset.item_ids.map(len).sum()\n",
        "\n",
        "# Preprocessing the data for the sparse user-item matrix\n",
        "user_coo = np.zeros(total_interactions_count, dtype=np.int64)\n",
        "item_coo = np.zeros(total_interactions_count, dtype=np.int64)\n",
        "pos = 0\n",
        "\n",
        "for user_id, item_ids in enumerate(tqdm(dataset.item_ids)):\n",
        "    user_coo[pos : pos + len(item_ids)] = user_id\n",
        "    item_coo[pos : pos + len(item_ids)] = item_ids\n",
        "    pos += len(item_ids)\n",
        "\n",
        "# Create the user-item sparse matrix in COO format and then convert to CSR\n",
        "shape = (max(user_coo) + 1, max(item_coo) + 1)\n",
        "user_item_matrix = sp.coo_matrix(\n",
        "    (np.ones(len(user_coo)), (user_coo, item_coo)), shape=shape\n",
        ")\n",
        "user_item_matrix = user_item_matrix.tocsr()\n",
        "\n",
        "# Save the matrix for later use\n",
        "sp.save_npz(\"data_train.npz\", user_item_matrix)\n",
        "# Cleanup memory. Later you need just data_train.npz\n",
        "del user_coo\n",
        "del item_coo\n",
        "del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b694e04f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-07T21:32:04.714869Z",
          "start_time": "2023-06-07T21:32:04.568491Z"
        },
        "execution": {
          "iopub.execute_input": "2024-03-21T10:27:58.137388Z",
          "iopub.status.busy": "2024-03-21T10:27:58.136543Z",
          "iopub.status.idle": "2024-03-21T10:27:58.300332Z",
          "shell.execute_reply": "2024-03-21T10:27:58.299898Z",
          "shell.execute_reply.started": "2024-03-21T10:27:58.137337Z"
        },
        "id": "b694e04f"
      },
      "outputs": [],
      "source": [
        "# you could start here if you already done precomputing\n",
        "# Load the sparse matrix for use in models\n",
        "user_item_matrix = sp.load_npz(\"data_train.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b7d5e4d1-cfcd-4abd-8d7e-b920bd3295b1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T10:38:33.146072Z",
          "iopub.status.busy": "2024-03-21T10:38:33.145637Z",
          "iopub.status.idle": "2024-03-21T10:38:33.153440Z",
          "shell.execute_reply": "2024-03-21T10:38:33.151232Z",
          "shell.execute_reply.started": "2024-03-21T10:38:33.146050Z"
        },
        "id": "b7d5e4d1-cfcd-4abd-8d7e-b920bd3295b1"
      },
      "outputs": [],
      "source": [
        "# Function to report sparse matrix size\n",
        "def sparce_matrix_report(matrix):\n",
        "    print('Size of raw data:', matrix.data.nbytes / 10**6, 'Mb')\n",
        "    print('Feedback matrix size:', matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d3992814-d1da-48c9-b42e-0c8b596de5ad",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T10:38:33.823522Z",
          "iopub.status.busy": "2024-03-21T10:38:33.822742Z",
          "iopub.status.idle": "2024-03-21T10:38:33.828502Z",
          "shell.execute_reply": "2024-03-21T10:38:33.827613Z",
          "shell.execute_reply.started": "2024-03-21T10:38:33.823495Z"
        },
        "id": "d3992814-d1da-48c9-b42e-0c8b596de5ad",
        "outputId": "b76e18d6-03c2-486a-f191-0f97f10dddd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of raw data: 46.339384 Mb\n",
            "Feedback matrix size: (75910, 104503)\n"
          ]
        }
      ],
      "source": [
        "sparce_matrix_report(user_item_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4cd37f54",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-07T21:32:24.023986Z",
          "start_time": "2023-06-07T21:32:23.942940Z"
        },
        "execution": {
          "iopub.execute_input": "2024-03-21T10:38:46.019297Z",
          "iopub.status.busy": "2024-03-21T10:38:46.018979Z",
          "iopub.status.idle": "2024-03-21T10:38:46.119795Z",
          "shell.execute_reply": "2024-03-21T10:38:46.119352Z",
          "shell.execute_reply.started": "2024-03-21T10:38:46.019277Z"
        },
        "id": "4cd37f54"
      },
      "outputs": [],
      "source": [
        "# Item weight distribution (for debugging)\n",
        "item_weights = np.array(user_item_matrix.tocsc().sum(0))[0]\n",
        "top_to_bottom_order = np.argsort(-item_weights)\n",
        "item_mapping = np.empty(top_to_bottom_order.shape, dtype=int)\n",
        "item_mapping[top_to_bottom_order] = np.arange(len(top_to_bottom_order))\n",
        "\n",
        "# Define a function to build a debug dataset for faster testing\n",
        "def build_debug_dataset(user_item_matrix, item_pct: float, user_pct: float):\n",
        "    '''Get given percent of top rated items and given percent of random users'''\n",
        "    total_item_count = (item_weights > 0).sum()\n",
        "    total_user_count = user_item_matrix.shape[0]\n",
        "\n",
        "    user_count = int(total_user_count * user_pct),\n",
        "    item_count = int(total_item_count * item_pct)\n",
        "    item_ids = top_to_bottom_order[:item_count]\n",
        "    user_ids = np.random.choice(\n",
        "        np.arange(user_item_matrix.shape[0]), size=user_count, replace=False\n",
        "    )\n",
        "    train = user_item_matrix[user_ids]\n",
        "    train = train[:, item_ids]\n",
        "    return train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "820abbd7-d8f2-42df-aa6a-e3a6e360c94d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-07T21:32:24.803339Z",
          "start_time": "2023-06-07T21:32:24.782858Z"
        },
        "execution": {
          "iopub.execute_input": "2024-03-21T10:39:09.832524Z",
          "iopub.status.busy": "2024-03-21T10:39:09.831807Z",
          "iopub.status.idle": "2024-03-21T10:39:09.855133Z",
          "shell.execute_reply": "2024-03-21T10:39:09.854617Z",
          "shell.execute_reply.started": "2024-03-21T10:39:09.832486Z"
        },
        "id": "820abbd7-d8f2-42df-aa6a-e3a6e360c94d",
        "outputId": "058d482a-949a-4162-83c7-9d274d7f444a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of raw data: 1.095792 Mb\n",
            "Feedback matrix size: (3795, 5019)\n"
          ]
        }
      ],
      "source": [
        "debug_dataset = build_debug_dataset(user_item_matrix, 0.05, 0.05)\n",
        "sparce_matrix_report(debug_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f5d59e1",
      "metadata": {
        "id": "8f5d59e1"
      },
      "source": [
        "This is useful for debugging (just to save time).\n",
        "\n",
        "**Final answers should use full dataset!!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "536eb9ab",
      "metadata": {
        "id": "536eb9ab"
      },
      "source": [
        "## Split dataset matrix (5 points)\n",
        "\n",
        "in the following way: for 20% of users (random) remove one like - this will be test data. The rest is train data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "95714ebd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-06T04:49:46.392558Z",
          "start_time": "2023-11-06T04:49:46.385337Z"
        },
        "id": "95714ebd"
      },
      "outputs": [],
      "source": [
        "def split_data(ratings):\n",
        "    # your code here\n",
        "    \"\"\"\n",
        "    Разделение матрицы рейтингов: для 20% пользователей удаляем один лайк\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import scipy.sparse as sp\n",
        "    \n",
        "    # Choose 20% random users for testing, remove one like for each\n",
        "    n_users = ratings.shape[0]\n",
        "    n_test_users = int(0.2 * n_users)\n",
        "    test_user_indices = np.random.choice(n_users, n_test_users, replace=False)\n",
        "    \n",
        "    # Initialize test matrix\n",
        "    test_data = []\n",
        "    test_row = []\n",
        "    test_col = []\n",
        "    \n",
        "    # Create a copy for the training matrix\n",
        "    train_matrix = ratings.copy()\n",
        "    \n",
        "    for user_idx in test_user_indices:\n",
        "        # Get items the user interacted with\n",
        "        user_items = ratings.getrow(user_idx).indices\n",
        "        \n",
        "        if len(user_items) > 0:\n",
        "            # Randomly remove one item for the test set\n",
        "            item_idx = np.random.choice(user_items)\n",
        "            \n",
        "            # Add to the test set\n",
        "            test_data.append(1.0)  # Бинарные взаимодействия\n",
        "            test_row.append(user_idx)\n",
        "            test_col.append(item_idx)\n",
        "            \n",
        "            # Remove from the training set\n",
        "            train_matrix[user_idx, item_idx] = 0\n",
        "    \n",
        "    # Создаем тестовую матрицу\n",
        "    test_matrix = sp.coo_matrix((test_data, (test_row, test_col)), shape=ratings.shape)\n",
        "    test_matrix = test_matrix.tocsr()\n",
        "    \n",
        "    # Убеждаемся, что тренировочная матрица в формате CSR\n",
        "    train_matrix = train_matrix.tocsr()\n",
        "    train_matrix.eliminate_zeros()  # Удаляем нулевые элементы\n",
        "    \n",
        "    return train_matrix, test_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e7bbfe9c",
      "metadata": {
        "id": "e7bbfe9c"
      },
      "outputs": [],
      "source": [
        "train_ratings, test_ratings = split_data(user_item_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8ed4c71-6501-4d34-951b-faf6fd4ce897",
      "metadata": {
        "id": "e8ed4c71-6501-4d34-951b-faf6fd4ce897"
      },
      "source": [
        "## Implement IALS (10 points each)\n",
        "\n",
        "Note that due to size of data you need to implement algorithm with _sparce matrices_!\n",
        "\n",
        "You are welcome to use classes like on the seminar:)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8a751bd1",
      "metadata": {
        "id": "8a751bd1"
      },
      "outputs": [],
      "source": [
        "# Function for GPU-accelerated IALS (Implicit Alternating Least Squares)\n",
        "def ials_gpu(ratings, k=40, lam=0.1, n_iterations=10, alpha=40):\n",
        "    '''GPU-accelerated Implicit Alternating Least Squares algorithm'''\n",
        "    import cupy as cp\n",
        "    import numpy as np\n",
        "    from tqdm.notebook import tqdm\n",
        "    \n",
        "    # Переносим данные на GPU\n",
        "    num_users, num_items = ratings.shape\n",
        "    \n",
        "    # Инициализируем факторы на GPU\n",
        "    user_embeddings = cp.random.normal(0, 0.01, (num_users, k)).astype(cp.float32)\n",
        "    item_embeddings = cp.random.normal(0, 0.01, (num_items, k)).astype(cp.float32)\n",
        "    \n",
        "    # Формат CSR для эффективной работы\n",
        "    ratings_csr = ratings.tocsr()\n",
        "    ratings_csc = ratings.tocsc()\n",
        "    \n",
        "    # Идентичная матрица для регуляризации на GPU\n",
        "    lambda_I = cp.eye(k, dtype=cp.float32) * lam\n",
        "    \n",
        "    for _ in tqdm(range(n_iterations), desc=\"IALS iterations\"):\n",
        "        # Step 1: Fix item factors and solve for user factors\n",
        "        YtY = cp.dot(item_embeddings.T, item_embeddings)\n",
        "        \n",
        "        for u in range(num_users):\n",
        "            items = ratings_csr[u].indices            \n",
        "            if len(items) == 0:\n",
        "                continue\n",
        "                \n",
        "            factors = cp.array(item_embeddings[items].get(), dtype=cp.float32)\n",
        "            confidence = 1.0 + alpha\n",
        "            \n",
        "            # Решаем систему на GPU\n",
        "            A = YtY + cp.dot(factors.T, (confidence - 1.0) * factors) + lambda_I\n",
        "            b = confidence * (cp.sum(factors, axis=0))\n",
        "            \n",
        "            try:\n",
        "                user_embeddings[u] = cp.linalg.solve(A, b)\n",
        "            except cp.linalg.LinAlgError:\n",
        "                user_embeddings[u] = cp.linalg.lstsq(A, b)[0]\n",
        "        \n",
        "        # Step 2: Fix user factors and solve for item factors\n",
        "        XtX = cp.dot(user_embeddings.T, user_embeddings)\n",
        "        \n",
        "        for i in range(num_items):\n",
        "            users = ratings_csc[:, i].indices\n",
        "            \n",
        "            if len(users) == 0:\n",
        "                continue\n",
        "                \n",
        "            factors = cp.array(user_embeddings[users].get(), dtype=cp.float32)\n",
        "            confidence = 1.0 + alpha\n",
        "            \n",
        "            A = XtX + cp.dot(factors.T, (confidence - 1.0) * factors) + lambda_I\n",
        "            b = confidence * (cp.sum(factors, axis=0))\n",
        "            \n",
        "            try:\n",
        "                item_embeddings[i] = cp.linalg.solve(A, b)\n",
        "            except cp.linalg.LinAlgError:\n",
        "                item_embeddings[i] = cp.linalg.lstsq(A, b)[0]\n",
        "    \n",
        "    # Возвращаем результаты на CPU\n",
        "    return user_embeddings.get(), item_embeddings.get()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bd9b9fd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ials_predictions = ials_gpu(train_ratings, k=40, lam= 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce1d293f",
      "metadata": {
        "id": "ce1d293f"
      },
      "source": [
        "## Compute MRR@100 metric for test users\n",
        "\n",
        "For ALS and IALS algorithms.\n",
        "\n",
        "**Don't forget to use full dataset!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ef8cdb0d",
      "metadata": {
        "id": "ef8cdb0d"
      },
      "outputs": [],
      "source": [
        "def mrr_gpu(user_embeddings, item_embeddings, test_ratings, train_ratings, k=100):\n",
        "    \"\"\"GPU-accelerated MRR calculation\"\"\"\n",
        "    import cupy as cp\n",
        "    import numpy as np\n",
        "    \n",
        "    # Переносим эмбеддинги на GPU\n",
        "    user_embeddings_gpu = cp.array(user_embeddings)\n",
        "    item_embeddings_gpu = cp.array(item_embeddings)\n",
        "    \n",
        "    test_ratings_csr = test_ratings.tocsr()\n",
        "    train_ratings_csr = train_ratings.tocsr()\n",
        "    \n",
        "    test_users = np.unique(test_ratings_csr.nonzero()[0])\n",
        "    \n",
        "    rr_sum = 0.0\n",
        "    count = 0\n",
        "    \n",
        "    for user in test_users:\n",
        "        test_items = test_ratings_csr[user].indices        \n",
        "        if len(test_items) == 0:\n",
        "            continue\n",
        "            \n",
        "        train_items = train_ratings_csr[user].indices\n",
        "        \n",
        "        # Вычисляем скоры на GPU\n",
        "        user_emb = user_embeddings_gpu[user]\n",
        "        scores = cp.dot(user_emb, item_embeddings_gpu.T)\n",
        "        \n",
        "        # Маскируем тренировочные элементы\n",
        "        scores_np = scores.get()\n",
        "        scores_np[train_items] = -np.inf\n",
        "        \n",
        "        # Получаем топ-k элементов\n",
        "        top_items = np.argsort(-scores_np)[:k]\n",
        "        \n",
        "        for test_item in test_items:\n",
        "            rank_idx = np.where(top_items == test_item)[0]\n",
        "            \n",
        "            if len(rank_idx) > 0:\n",
        "                rank = rank_idx[0] + 1\n",
        "                rr_sum += 1.0 / rank\n",
        "                count += 1\n",
        "    \n",
        "    if count > 0:\n",
        "        mrr_value = rr_sum / count\n",
        "    else:\n",
        "        mrr_value = 0.0\n",
        "        \n",
        "    return mrr_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8615c7d5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5db5dbfbc03c4f7d93d61de2b630acc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "IALS iterations:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train IALS with GPU\n",
        "user_embeddings_gpu, item_embeddings_gpu = ials_gpu(train_ratings, k=40, lam=0.1, n_iterations=10, alpha=40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e3249ab2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MRR@100: 0.0911\n"
          ]
        }
      ],
      "source": [
        "# Compute MRR for the model\n",
        "mrr_value = mrr_gpu(user_embeddings_gpu, item_embeddings_gpu, test_ratings, train_ratings, k=100)\n",
        "print(f\"MRR@100: {mrr_value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa82ca33",
      "metadata": {
        "id": "aa82ca33"
      },
      "outputs": [],
      "source": [
        "def get_ials_similarities_sparse_gpu(item_embeddings, batch_size=1000):\n",
        "    import cupy as cp\n",
        "    from scipy.sparse import lil_matrix\n",
        "    \n",
        "    # Ensure that item_embeddings is a CuPy array\n",
        "    item_embeddings = cp.asarray(item_embeddings)  # Convert to CuPy array if it's a NumPy array\n",
        "    \n",
        "    num_items = item_embeddings.shape[0]\n",
        "    \n",
        "    # Normalize the embeddings to prevent large values in the dot product\n",
        "    norms = cp.linalg.norm(item_embeddings, axis=1, keepdims=True)\n",
        "    normalized_embeddings = item_embeddings / (norms + 1e-8)\n",
        "    \n",
        "    # Initialize a sparse similarity matrix in LIL format (efficient for incremental construction)\n",
        "    similarities = lil_matrix((num_items, num_items), dtype=cp.float32)\n",
        "    \n",
        "    # Process in batches to avoid memory issues\n",
        "    for start_idx in range(0, num_items, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, num_items)\n",
        "        \n",
        "        # Slice the batch of embeddings\n",
        "        batch_embeddings = normalized_embeddings[start_idx:end_idx]\n",
        "        \n",
        "        # Compute the dot product for the batch with only the items in the batch\n",
        "        batch_similarities = cp.dot(batch_embeddings, normalized_embeddings.T)\n",
        "        \n",
        "        # Store only non-zero similarities in the sparse matrix\n",
        "        for i in range(batch_similarities.shape[0]):\n",
        "            for j in range(batch_similarities.shape[1]):\n",
        "                if batch_similarities[i, j] != 0:  # Only store non-zero similarities\n",
        "                    similarities[start_idx + i, j] = batch_similarities[i, j].get()  # Convert to NumPy\n",
        "    \n",
        "    # Set diagonal to zero (self-similarity)\n",
        "    cp.fill_diagonal(similarities, 0)\n",
        "    \n",
        "    # Return the computed similarities as a sparse matrix on CPU\n",
        "    return similarities\n",
        "\n",
        "# Example of computing the item-item similarity matrix using sparse format\n",
        "item_similarities_sparse = get_ials_similarities_sparse_gpu(item_embeddings_gpu, batch_size=500)\n",
        "\n",
        "# Output the similarity matrix\n",
        "print(item_similarities_sparse)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ec74ade",
      "metadata": {
        "id": "0ec74ade"
      },
      "source": [
        "## Adjust hyperparameters of IALS to maximize MRR (10 points)\n",
        "\n",
        "Main hyperparameters are regularization and weights for implicit case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "d5558e37",
      "metadata": {
        "id": "d5558e37"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ea3d1c3",
      "metadata": {
        "id": "9ea3d1c3"
      },
      "source": [
        "\n",
        "Optimal parameters of IALS are:\n",
        "\n",
        "...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c32c4d60",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4b1bfd4f",
      "metadata": {
        "id": "4b1bfd4f"
      },
      "source": [
        "## Get similarities from item2item CF (10 points)\n",
        "\n",
        "Item2item can be taken from the first homework, SLIM was implemented in the class.\n",
        "\n",
        "Alternatively you could use libraries, but in this case you will need to convert dataset to their format.\n",
        "\n",
        "You need to compute only item similarities, not predictions for users."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d6b19e8",
      "metadata": {
        "id": "2d6b19e8"
      },
      "source": [
        "## Compare similarities from four algorithms (20 points)\n",
        "\n",
        "* plot distributions\n",
        "* compute metrics (which you think are relevant)\n",
        "* look at several top similar lists\n",
        "\n",
        "Make conclusion how these methods differ in computing similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a7d9d8d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-06T05:10:53.235810Z",
          "start_time": "2023-11-06T05:10:53.229989Z"
        },
        "id": "4a7d9d8d"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5544dc69",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-06T05:12:40.299803Z",
          "start_time": "2023-11-06T05:12:40.290403Z"
        },
        "id": "5544dc69"
      },
      "source": [
        "Conclusion:\n",
        "\n",
        "...."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "finalized": {
      "timestamp": 1686173501421,
      "trusted": false
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "302.398px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
