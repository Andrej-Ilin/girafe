# -*- coding: utf-8 -*-
"""lab02_boosting_and_model_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fKqYST9v_9vEjXEwbScJ9CUaCwxKZ10k

# <a href="https://girafe.ai/" target="_blank" rel="noopener noreferrer"><img src="https://raw.githubusercontent.com/girafe-ai/ml-course/7096a5df4cada5ee651be1e3215c2f7fb8a7e0bf/logo_margin.svg" alt="girafe-ai logo" width="150px" align="left"></a> [ml-basic course](https://github.com/girafe-ai/ml-course) <a class="tocSkip">

# Lab assignment â„–2
## Gradient boosting and model analysis

Today we will work with Gradient Boosting library. It is one of the most popular models these days that shows both great quality and performance.

Choises for library are:

* [LightGBM](https://github.com/Microsoft/LightGBM) by Microsoft. Handful and fast.
* [Catboost](https://github.com/catboost/catboost) by Yandex. Tuned to deal well with categorical features.
* [xgboost](https://github.com/dmlc/xgboost) by dlmc. The most famous framework which got very popular on kaggle.

**Dataset**

By default we will work with widely known [Human Actividy Recognition (HAR) dataset](https://archive.ics.uci.edu/dataset/240/human+activity+recognition+using+smartphones). Data is available at UCI repository.

There are available both raw and preprocessed datasets. This time we will use the preprocessed one.
Some simple preprocessing is done for you.

If you want more interpretable data, you can take [Wine quality dataset](https://archive.ics.uci.edu/dataset/186/wine+quality) (see details below).

Your __ultimate target is to get familiar with one of the frameworks above__ and achieve at least 90% accuracy on test dataset and try to get some useful insights on the features the model paid attention to.

_Despite the main language of this notebook is English, feel free to write your thoughts in Russian if it's more convenient for you._

## Part 0. Downloading and preprocessing

The preprocessing is done for you. Let's take a look at the data:
"""

# Download and unpack dataset from UCI
!wget -nc https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip
!unzip -q -u "UCI HAR Dataset.zip"

from pathlib import Path
from typing import Union

import matplotlib.pyplot as plt
import numpy as np

def read_har_labels(path: Path) -> dict[int, str]:
    labels = {}
    with path.open() as file:
        for line in file:
            label, name = line.strip().split(" ")
            labels[int(label)] = name

    return labels


def read_har(path: Union[str, Path]) -> tuple:
    path = Path(path)

    activity_labels = read_har_labels(path / "activity_labels.txt")
    feature_labels = read_har_labels(path / "features.txt")

    X_train = np.genfromtxt(path / "train/X_train.txt")
    y_train = np.genfromtxt(path / "train/y_train.txt")
    X_test = np.genfromtxt(path / "test/X_test.txt")
    y_test = np.genfromtxt(path / "test/y_test.txt")

    return activity_labels, feature_labels, X_train, y_train, X_test, y_test

activity_labels, feature_labels, X_train, y_train, X_test, y_test = read_har("UCI HAR Dataset")
n_features = X_train.shape[1]

print(f"Train set: {X_train.shape}, {y_train.shape}")
print(f"Test set: {X_test.shape}, {y_test.shape}")
print("Activity labels: ", activity_labels)

"""### Alternative dataset: Wine quality

Please, take this dataset if you are sure you can preprocess it yourself and ready to work with it's features and results.

However you will have more interpretable features which can be analysed with shap in last part.
"""

!pip install ucimlrepo

import ucimlrepo as uci

dataset = uci.fetch_ucirepo(id=186)

print(dataset.metadata.name, '\n')
print(dataset.metadata.abstract, '\n')
print(dataset.metadata.additional_info.summary, '\n')

"""### Analyse and preprocess data

First you need to analyse data you have (including performing EDA in a section below).

Get insights on the data you have by calculating basic statistics, plotting data and so on.

_Tip: data may contain some nasty artefacts. So, don't hesitate to deal with them._
"""

# YOUR CODE HERE

"""Apply PCA to the data"""

from sklearn.decomposition import PCA

# YOUR CODE HERE

X_train_pca.shape, X_test_pca.shape

plt.scatter(X_train_pca[:1000, 0], X_train_pca[:1000, 1], c=y_train[:1000])
plt.grid()
plt.xlabel("Principal component 1")
plt.ylabel("Principal component 2")

plt.scatter(X_train_pca[:1000, 3], X_train_pca[:1000, 4], c=y_train[:1000])
plt.grid()
plt.xlabel("Principal component 4")
plt.ylabel("Principal component 5")

"""## Part 1: Fit the model

Rough example for that part: https://rpubs.com/burakh/har_xgb

Write basic model building and validation procedure. At this point you don't need to pick hyperparameters intensively. You may check a few by hand if you want. Use original feature set.
"""

# YOUR CODE HERE

"""### Metrics and plots

Calculate metrics for classification problem which we discussed on lecture.

Plot visualizations needed.
"""

# YOUR CODE HERE

"""### Conclusion on basic model results

Describe what do you see in data you acquired, what can you say about that model?

**YOUR TEXT HERE**

## Part 2: Use hyper parameter tuning system

Use [optuna](https://optuna.org/) or [hyperopt](http://hyperopt.github.io/hyperopt/) zero order optimizer to find optimal hyper param set.
"""

# YOUR CODE HERE

"""### Conclusion

Please, write down your thoughts on the experiment results (which hparams are more important than others, which improve did you get, etc):

**YOUR TEXT HERE**

## Part 3. Interpret the model predictions

Train model with the best hparams you've found and analyze it.

Please use [shap](https://github.com/slundberg/shap) to build beeswarm plot and at least one more of your choice and interpret them (in a conclusion section below).

Feature names were presented in dataset loading function.|
"""

import shap  # noqa: F401


# YOUR CODE HERE

"""_Tip for HAR_: there are duplicating features. You may describe their behaviour as part of conclusion."""

# fmt: off
duplicating_columns = (
    205, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 231, 244, 257, 507, 520, 533, 546,
)
# fmt: on

duplicating_mask = np.isin(range(n_features), duplicating_columns)

"""### Conclusion

Your thoughts about the plots and model behaviour (not limited to duplicating features):

**YOUR TEXT HERE**
"""