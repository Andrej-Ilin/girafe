{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Credits: materials from this notebook belong to YSDA [Practical DL](https://github.com/yandexdataschool/Practical_DL) course. Special thanks for making them available online.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab assignment №1, part 1\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matrix differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: \n",
    "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "y = x^Tx,  \\quad x \\in \\mathbb{R}^N \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} = \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Определение функции**:\n",
    "   $$\n",
    "   y = x^T x\n",
    "   $$\n",
    "   Здесь $ x^T $ — это транспонированный вектор $ x $, и результатом является скалярное произведение вектора на самого себя, что можно записать как:\n",
    "   $$\n",
    "   y = \\sum_{i=1}^{N} x_i^2\n",
    "   $$\n",
    "\n",
    "2. **Производная функции**:\n",
    "   Мы хотим найти производную $ \\frac{dy}{dx} $. Для этого воспользуемся правилом дифференцирования. Мы можем использовать свойство градиента для векторных функций.\n",
    "\n",
    "   Градиент функции $ y = x^T x $ по вектору $ x $ можно вычислить следующим образом:\n",
    "   $$\n",
    "   \\nabla_x (x^T x) = \\nabla_x \\left( \\sum_{i=1}^{N} x_i^2 \\right)\n",
    "   $$\n",
    "\n",
    "   Для каждого элемента $ x_i $ производная будет равна:\n",
    "   $$\n",
    "   \\frac{\\partial y}{\\partial x_i} = 2x_i\n",
    "   $$\n",
    "\n",
    "   Таким образом, градиент будет:\n",
    "   $$\n",
    "   \\nabla_x (x^T x) = \\begin{bmatrix}\n",
    "   2x_1 \\\\\n",
    "   2x_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   2x_N\n",
    "   \\end{bmatrix} = 2x\n",
    "   $$\n",
    "\n",
    "3. **Ответ**:\n",
    "   Производная $ \\frac{dy}{dx} $ векторной функции $ y = x^T x $ будет равна:\n",
    "   $$\n",
    "   \\boxed{\\frac{dy}{dx} = 2x}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dA} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\text{tr} $ обозначает след матрицы, который равен сумме диагональных элементов матрицы например $ tr(A) = \\sum_{j} A_{jj} = \\sum{\\lambda_j} $.\n",
    "\n",
    "### Использование свойства следа\n",
    "\n",
    "Также существует важное *свойство следа, которое говорит, что:\n",
    "\n",
    "$$\n",
    "\\text{tr}(AB) = \\text{tr}(BA)\n",
    "$$\n",
    "\n",
    "Таким образом, мы можем переписать $ y $ как:\n",
    "\n",
    "$$\n",
    "y = \\text{tr}(AB) = \\text{tr}(BA)\n",
    "$$\n",
    "\n",
    "### Вычисление производной\n",
    "\n",
    "Чтобы найти производную $ \\frac{dy}{dA} $, воспользуемся известным результатом о производной следа:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dA} \\text{tr}(AB) = B^T\n",
    "$$\n",
    "\n",
    "### Ответ:\n",
    "\n",
    "Производная функции $ y = \\text{tr}(AB) $ по матрице $ A $ будет равна:\n",
    "\n",
    "$$\n",
    "\\boxed{\\frac{dy}{dA} = B^T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "*Комментарий: Производная $\\frac{dy}{dA} = B^\\top $ напрямую связана с нашим замечанием о свойстве следа $\\text{tr}(AB) = \\text{tr}(BA) $. Рассмотрим оба представления:1. Если записать $y = \\text{tr}(AB) $, то при дифференцировании по $A $ каждый элемент $A_{ik} $ умножается на $B_{ki} $ (что дает $B^\\top $).2. Если использовать $y = \\text{tr}(BA) $, структура не меняется: производная по $A $ всё равно «вытягивает» коэффициенты при элементах $A $, которые расположены как $B^\\top $.Таким образом, свойство следа подтверждает результат, но не отменяет необходимость транспонирования $B $ в производной. Пример: для $A, B $ размерности2×2, $\\frac{\\partial y}{\\partial A_{12}} = B_{21} $, что соответствует $B^\\top $ на позиции (1,2).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dA} =\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint for the latter (one of the ways): use *ex. 2* result and the fact \n",
    "$$\n",
    "tr(ABC) = tr (CAB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ y = x^T A c $. Это скалярное значение, так как $x^T \\in \\mathbb{R}^{1 \\times N}$, $A \\in \\mathbb{R}^{N \\times N}$ и $c \\in \\mathbb{R}^{N}$, то произведение $x^T A c$ будет скаляром.\n",
    "\n",
    "Чтобы вычислить $\\frac{dy}{dx}$, нужно взять производную $y = x^T A c$ по $x$. Так как $A$ и $c$ являются постоянными величинами, то $A c$ рассматривается как постоянный вектор.\n",
    "\n",
    "Производная вычисляется следующим образом:\n",
    "\n",
    "$\\frac{dy}{dx} = \\frac{d}{dx} \\left( x^T A c \\right) = A c$\n",
    "\n",
    "### Ответ:\n",
    "\n",
    "Производная $y$ по $x$ равна: $\\frac{dy}{dx} = A c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переишем $y$ через след: $ y = x^T A c = tr(x^T A c)$ (и используая подсказку можем продолижить) $y = tr(c x^T A)$ \n",
    "\n",
    "Заменим $ c x^T $ на B, то получим следуеющее выражение $y = tr(B A)$  и используем свойство из решения второго примера $\\frac{dy}{dA} = B^T$\n",
    "\n",
    "Здесь $B = c x^T$, значит $\\frac{dy}{dA} = (c x^T)^T = x c^T$ \n",
    "\n",
    "### Ответ:\n",
    "\n",
    "$$\n",
    "\\boxed{\\frac{dy}{dA} = x c^T}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic matrix factorization example. Given matrix $X$ you need to find $A$, $S$ to approximate $X$. This can be done by simple gradient descent iteratively alternating $A$ and $S$ updates.\n",
    "$$\n",
    "J = || X - AS ||_F^2  , \\quad A\\in \\mathbb{R}^{N \\times R} , \\quad S\\in \\mathbb{R}^{R \\times M}\n",
    "$$\n",
    "$$\n",
    "\\frac{dJ}{dS} = ? \n",
    "$$\n",
    "\n",
    "You may use one of the following approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First approach\n",
    "Using ex.2 and the fact:\n",
    "$$\n",
    "|| X ||_F^2 = tr(XX^T) \n",
    "$$ \n",
    "it is easy to derive gradients (you can find it in one of the refs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы найти $\\frac{dJ}{dS}$ используем подход на следе матрицы. \n",
    "\n",
    "$J = || X - AS ||_F^2 = tr((X - AS)(X - AS)T)$ \n",
    "\n",
    "Раскроем выражение: \n",
    "\n",
    "$J = tr(X X^T) - tr(X S^T A^T) - tr(A S X^T) + tr(A S S^T A^T)$\n",
    "\n",
    "Теперь найдем производную $\\frac{dJ}{dS}$ , учитывая линейность следа и его свойства:\n",
    "\n",
    "Производная $\\frac{dJ}{dS}$ включает только члены, зависящие от $S$. Члены с $tr(X X^T)$ не зависят от $S$, \n",
    "поэтому их производные по $S$ равны нулю и они исчезают.\n",
    "\n",
    "$\\frac{dJ}{dS} = \\frac{d}{dS} (- tr(X S^T A^T) - tr(A S X^T) + tr(A S S^T A^T))$ \n",
    "\n",
    "1. $- tr(X S^T A^T) =  - A^T X $ по аналогии с примером $\\frac{d}{dS} tr(B S^T) = B$\n",
    "\n",
    "2. $- tr(A S X^T) = -A^T X$ через перестановку следа $ tr(AS X^T) = tr(X^T AS) = tr(S X^T A)$\n",
    "\n",
    "3. Чтобы найти $\\frac{d}{dS}tr(A S S^T A^T), нужно раскрыть след и продифиринцировать по шагово:\n",
    "\n",
    "    Циклическа перестановка $tr(A S S^T A^T) = tr(S^T A^T AS)$ , так как $tr(XYZ) = tr(ZXY)$. \n",
    "\n",
    "    Пусть $C = A^T A$ (симетричная матрица). Тогда выражение принимает вид $tr(S^T CS)$  \n",
    "\n",
    "    Тогда производная квадратичной формы принимает вид $\\frac{d}{dS}tr(S^T CS) = 2CS$, где $C = A^T A$.\n",
    "    \n",
    "    получаем $ tr(A S S^T A^T) = 2A^T AS$\n",
    "\n",
    "Собираем все вместе $\\frac{dJ}{dS} = - A^T X - A^T X + 2A^T AS = -2A^T X + 2A^T AS$ \n",
    "\n",
    "Вынесем за скобки $2A^T$ получаем  \n",
    "\n",
    "#### Ответ: \n",
    "\n",
    "$$\\boxed{\\frac{dJ}{dS} = 2A^T(AS - X)}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second approach\n",
    "You can use *slightly different techniques* if they suits you. Take a look at this derivation:\n",
    "<img src=\"grad.png\">\n",
    "(excerpt from [Handbook of blind source separation, Jutten, page 517](https://books.google.ru/books?id=PTbj03bYH6kC&printsec=frontcover&dq=Handbook+of+Blind+Source+Separation&hl=en&sa=X&ved=0ahUKEwi-q_apiJDLAhULvXIKHVXJDWcQ6AEIHDAA#v=onepage&q=Handbook%20of%20Blind%20Source%20Separation&f=false), open for better picture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third approach\n",
    "And finally we can use chain rule! \n",
    "let $ F = AS $ \n",
    "\n",
    "**Find**\n",
    "$$\n",
    "\\frac{dJ}{dF} =  \n",
    "$$ \n",
    "and \n",
    "$$\n",
    "\\frac{dF}{dS} =  \n",
    "$$ \n",
    "(the shape should be $ NM \\times RM $).\n",
    "\n",
    "Now it is easy do get desired gradients:\n",
    "$$\n",
    "\\frac{dJ}{dS} =  \n",
    "$$ \n",
    "\n",
    "### 1. Определение размеров матриц\n",
    "\n",
    "Итак, у нас есть три матрицы:\n",
    "\n",
    "- $ X $ размером $ N \\times M $,\n",
    "- $ A $ размером $ N \\times R $,\n",
    "- $ S $ размером $ R \\times M $.\n",
    "\n",
    "Произведение $ F = A S $ имеет размерность $ N \\times M $, так как:\n",
    "- $ A $ — матрица размером $ N \\times R $,\n",
    "- $ S $ — матрица размером $ R \\times M $.\n",
    "\n",
    "Таким образом, матрица $ F $ будет размером $ N \\times M $.\n",
    "\n",
    "### 2. Шаг 1: Производная функции стоимости по $ F $\n",
    "\n",
    "Функция стоимости $ J $ задана как $ J = \\| X - F \\|_F^2 $, где $ F = A S $. Чтобы найти производную функции стоимости по $ F $, применяем стандартное правило для квадратичной ошибки (матрица Фробениуса):\n",
    "\n",
    "$\n",
    "\\frac{dJ}{dF} = -2(X - F)\n",
    "$\n",
    "\n",
    "Это выражение имеет размерность $ N \\times M $, так как и $ X $, и $ F $ имеют такую же размерность.\n",
    "\n",
    "### 3. Шаг 2: Производная $ F $ по $ S $\n",
    "\n",
    "Теперь найдем производную $ F = A S $ по $ S $. Поскольку $ A $ — это матрица, умножающая $ S $, то:\n",
    "\n",
    "$\n",
    "\\frac{dF}{dS} = A\n",
    "$\n",
    "\n",
    "Матрица $ A $ имеет размерность $ N \\times R $, и именно её производная по $ S $ имеет такую же размерность.\n",
    "\n",
    "### 4. Шаг 3: Применение цепного правила\n",
    "\n",
    "Теперь применим цепное правило для вычисления производной функции стоимости по $ S $:\n",
    "\n",
    "$\n",
    "\\frac{dJ}{dS} = \\frac{dJ}{dF} \\cdot \\frac{dF}{dS}\n",
    "$\n",
    "\n",
    "Подставляем выражения, полученные на предыдущих шагах:\n",
    "\n",
    "$\n",
    "\\frac{dJ}{dS} = (-2(X - F)) \\cdot A\n",
    "$\n",
    "\n",
    "Где:\n",
    "- $ \\frac{dJ}{dF} = -2(X - F) $ имеет размерность $ N \\times M $,\n",
    "- $ \\frac{dF}{dS} = A $ имеет размерность $ N \\times R $.\n",
    "\n",
    "Чтобы перемножить эти матрицы, нужно учесть размерности и транспонировать матрицу $ A $, так как при умножении $ (N \\times M) \\cdot (N \\times R) $ результат не будет иметь нужную размерность. Транспонированная матрица $ A^T $ имеет размерность $ R \\times N $, и её умножение на выражение $ (X - F) $ даст нужный результат.\n",
    "\n",
    "Таким образом, окончательная производная будет:\n",
    "\n",
    "$\n",
    "\\frac{dJ}{dS} = 2 A^T (A S - X)\n",
    "$\n",
    "\n",
    "#### Ответ:\n",
    "\n",
    "$$\n",
    "\\boxed{\\frac{dJ}{dS} = 2 A^T (A S - X)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 2. kNN questions\n",
    "Here come the questions from the assignment0_01. Please, refer to the assignment0_01 to get the context of the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)\n",
    "\n",
    "- What in the data is the cause behind the distinctly bright rows?\n",
    "- What causes the columns?\n",
    "\n",
    "*Your Answer:*\n",
    "\n",
    "* The distinctly bright rows in the distance matrix correspond to test examples that are distant from most of the training data. These rows show high distances, which appear bright due to the color map where white represents higher values. It indicates that those particular test images are very dissimilar from the majority of the training examples.\n",
    "\n",
    "* The distinctly bright rows in the distance matrix correspond to test examples that are distant from most of the training data. These rows show high distances, which appear bright due to the color map where white represents higher values. It indicates that those particular test images are very dissimilar from the majority of the training examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "We can also use other distance metrics such as L1 distance.\n",
    "For pixel values $p_{ij}^{(k)}$ at location $(i,j)$ of some image $I_k$, \n",
    "\n",
    "the mean $\\mu$ across all pixels over all images is $$\\mu=\\frac{1}{nhw}\\sum_{k=1}^n\\sum_{i=1}^{h}\\sum_{j=1}^{w}p_{ij}^{(k)}$$\n",
    "And the pixel-wise mean $\\mu_{ij}$ across all images is \n",
    "$$\\mu_{ij}=\\frac{1}{n}\\sum_{k=1}^np_{ij}^{(k)}.$$\n",
    "The general standard deviation $\\sigma$ and pixel-wise standard deviation $\\sigma_{ij}$ is defined similarly.\n",
    "\n",
    "Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply.\n",
    "1. Subtracting the mean $\\mu$ ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu$.)\n",
    "#### Answer: Does not change performance\n",
    "Explanation: In L1 distance, subtracting the overall mean shifts all the data similarly and \n",
    "\n",
    "does not affect the relative distances between points. Therefore, it does not change the performance.\n",
    "\n",
    "2. Subtracting the per pixel mean $\\mu_{ij}$  ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu_{ij}$.)\n",
    "#### Answer: Does not change performance\n",
    "Explanation: In L1 distance, subtracting the per pixel mean can still preserve the relative differences \n",
    "\n",
    "between images, as it just centers the pixel values but does not change the relative distances for classification.\n",
    "\n",
    "3. Subtracting the mean $\\mu$ and dividing by the standard deviation $\\sigma$.\n",
    "#### Answer: Changes performance\n",
    "Explanation: While this is typically useful for Euclidean distance (as it normalizes the data), \n",
    "it may affect performance in L1 distance by altering the data distribution and introducing scaling effects, which might affect the accuracy.\n",
    "\n",
    "4. Subtracting the pixel-wise mean $\\mu_{ij}$ and dividing by the pixel-wise standard deviation $\\sigma_{ij}$.\n",
    "#### Answer: Changes performance\n",
    "Explanation: This normalization modifies the pixel values and changes the relative distances in the L1 space.\n",
    "\n",
    " The distances affected because the pixel values are standardized across images.\n",
    "5. Rotating the coordinate axes of the data.\n",
    "#### Answer: Changes performance\n",
    "\n",
    "Explanation: Rotating the coordinate axes will definitely affect the relative distances between points in feature space, altering the performance of the nearest neighbor classifier.\n",
    "\n",
    "\n",
    "*Your Answer:*\n",
    "\n",
    "\n",
    "*Your Explanation:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Which of the following statements about $k$-Nearest Neighbor ($k$-NN) are true in a classification setting, and for all $k$? Select all that apply.\n",
    "1. The decision boundary (hyperplane between classes in feature space) of the k-NN classifier is linear.\n",
    "#### Answer: False\n",
    "Explanation: The decision boundary in \n",
    "k\n",
    "k-NN is non-linear. The classifier makes decisions based on the local neighborhood of the test point, \n",
    "\n",
    "which leads to a decision boundary that can be very complex, depending on the distribution of the data.\n",
    "\n",
    "2. The training error of a 1-NN will always be lower than that of 5-NN.\n",
    "#### Answer: True\n",
    "\n",
    "Explanation: A 1-NN classifier will always have lower training error because it directly assigns the \n",
    "\n",
    "label of the nearest training point to each test example, and since it uses exact matches, the training error is minimized. \n",
    "\n",
    "In contrast, 5-NN introduces more flexibility, and some test points may be misclassified due to the majority voting from the 5 nearest neighbors.\n",
    "\n",
    "3. The test error of a 1-NN will always be lower than that of a 5-NN.\n",
    "#### Answer: False\n",
    "\n",
    "Explanation: A 1-NN classifier can be highly sensitive to noise and outliers, which may result in higher test error, \n",
    "\n",
    "especially in real-world datasets. A 5-NN classifier tends to smooth out this sensitivity by considering more neighbors, \n",
    "\n",
    "which often leads to better generalization and lower test error.\n",
    "\n",
    "4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set.\n",
    "#### Answer: True\n",
    "\n",
    "Explanation: In a \n",
    "k\n",
    "k-NN classifier, the time complexity for classifying a test point is proportional to the number of training points because the classifier computes the distance between the test point and every training point, which increases as the training set grows.\n",
    "\n",
    "\n",
    "5. None of the above.\n",
    "#### Answer: True\n",
    "\n",
    "Explanation: In a \n",
    "k\n",
    "k-NN classifier, the time complexity for classifying a test point is proportional to the number of training points because the classifier computes the distance between the test point and every training point, which increases as the training set grows.\n",
    "\n",
    "\n",
    "\n",
    "*Your Answer:*\n",
    "\n",
    "\n",
    "*Your Explanation:*\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mipt",
   "language": "python",
   "name": "mipt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
