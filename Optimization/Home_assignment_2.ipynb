{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tasks presented below derive the analytic expression of the requested objects. Do not\n",
    "forget to transform result in the vector form, i.e. the answer in the form “the k-th element of the\n",
    "gradient is ...” is not Ok. Please, derive the closed forms for the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (1.5 pts) Compute the gradients with respect to $ U \\in \\mathbb{R}^{n \\times k} $ and $ V \\in \\mathbb{R}^{k \\times n} $, $ k < n $, of the function\n",
    "$$\n",
    "J(U, V) =  \\| UV - Y \\|_F^2 + \\frac{\\lambda}{2} (\\| U \\|_F^2 + \\| V \\|_F^2),\n",
    "$$\n",
    "where $ \\lambda > 0 $ is a given number. Also, $ \\| A \\|_F = \\sqrt{\\sum_{i,j} a_{ij}^2} $ denotes the Frobenius norm of the matrix $ A = [a_{ij}] $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solving\n",
    "Gradient with respect to $ U $:\n",
    "Using the chain rule and properties of matrix derivatives, we have:\n",
    "$$\n",
    "\\nabla_U \\| UV - Y \\|_F^2 = 2 (UV - Y) V^T.\n",
    "$$\n",
    "For the regularization term:\n",
    "$$\n",
    "\\nabla_U \\left( \\frac{\\lambda}{2} \\| U \\|_F^2 \\right) = \\lambda U.\n",
    "$$\n",
    "Combining these results:\n",
    "$$\n",
    "\\nabla_U J(U, V) = 2 (UV - Y) V^T + \\lambda U.\n",
    "$$\n",
    "\n",
    "Gradient with respect to $ V $:\n",
    "Similarly, we can find the gradient with respect to $ V $:\n",
    "$$\n",
    "\\nabla_V \\| UV - Y \\|_F^2 = 2 U^T (UV - Y).\n",
    "$$\n",
    "And for the regularization term:\n",
    "$$\n",
    "\\nabla_V \\left( \\frac{\\lambda}{2} \\| V \\|_F^2 \\right) = \\lambda V.\n",
    "$$\n",
    "Combining these results:\n",
    "$$\n",
    "\\nabla_V J(U, V) = 2 U^T (UV - Y) + \\lambda V.\n",
    "$$\n",
    "\n",
    "Final Results for Problem 1:\n",
    "The gradients are:\n",
    "$$\n",
    "\\boxed{\\nabla_U J(U, V) = 2 (UV - Y) V^T + \\lambda U},\n",
    "$$\n",
    "$$\n",
    "\\boxed{\\nabla_V J(U, V) = 2 U^T (UV - Y) + \\lambda V}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. (1 pt) Compute the gradient of the following function\n",
    "$$\n",
    "f(w) = \\sum_{i=1}^{m} \\left( \\log(1 + e^{-y_i w^T x_i}) \\right) + \\frac{1}{2} \\| w \\|_2^2,\n",
    "$$\n",
    "where $ x_i \\in \\mathbb{R}^n $, $ y_i \\in \\mathbb{R} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solving\n",
    "Gradient of the first term:\n",
    "Using the chain rule and properties of derivatives, we can find:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w} \\log\\left(1 + e^{-y_i w^T x_i}\\right) = -y_i \\frac{e^{-y_i w^T x_i}}{1 + e^{-y_i w^T x_i}} x_i.\n",
    "$$\n",
    "This term represents the derivative of the logistic loss function. Summing over all $$ m $$:\n",
    "$$\n",
    "\\nabla_w \\sum_{i=1}^{m} \\log\\left(1 + e^{-y_i w^T x_i}\\right) = -\\sum_{i=1}^{m} y_i \\frac{e^{-y_i w^T x_i}}{1 + e^{-y_i w^T x_i}} x_i.\n",
    "$$\n",
    "This can be expressed in terms of the sigmoid function:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}.\n",
    "$$\n",
    "Therefore, the gradient becomes:\n",
    "$$\n",
    "-\\sum_{i=1}^{m} y_i \\sigma(-y_i w^T x_i) x_i.\n",
    "$$\n",
    "\n",
    "Gradient of the regularization term:\n",
    "The gradient of the regularization term is:\n",
    "$$\n",
    "\\nabla_w \\left(\\frac{1}{2} \\|w\\|_2^2\\right) = w.\n",
    "$$\n",
    "\n",
    "Step 2: Combine Results\n",
    "Putting both parts together gives:\n",
    "$$\n",
    "\\nabla f(w) = -\\sum_{i=1}^{m} y_i \\sigma(-y_i w^T x_i) x_i + w.\n",
    "$$\n",
    "\n",
    "Final Result:\n",
    "The gradient is:\n",
    "$$\n",
    "\\boxed{\\nabla f(w) = -\\sum_{i=1}^{m} y_i \\frac{e^{-y_i w^T x_i}}{1 + e^{-y_i w^T x_i}} x_i + w}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аннотация\n",
    "Норма Фробениуса — это обобщение евклидовой нормы на матрицы, и она определяется как квадратный корень из суммы квадратов всех элементов матрицы. Для матрицы $ A $ размером $ m \\times n $ норма Фробениуса может быть записана следующим образом:\n",
    "\n",
    "$$\n",
    "\\| A \\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2}\n",
    "$$\n",
    "\n",
    "где $ a_{ij} $ — элементы матрицы $ A $.\n",
    "\n",
    "Когда мы рассматриваем задачи оптимизации, в которых участвуют матрицы, например, в методах машинного обучения или в методах низкоранговой матричной факторизации, норма Фробениуса часто используется в качестве функции потерь. Например, если мы хотим минимизировать разницу между некоторой матрицей $ A $ и ее приближением $ \\hat{A} $ (например, через произведение матриц $ U $ и $ V $), мы можем использовать следующую функцию потерь:\n",
    "\n",
    "$$\n",
    "L(U, V) = \\| A - UV \\|_F^2\n",
    "$$\n",
    "\n",
    "Раскрывая квадрат нормы Фробениуса, мы получаем:\n",
    "\n",
    "$$\n",
    "L(U, V) = \\sum_{i,j} |a_{ij} - (UV)_{ij}|^2\n",
    "$$\n",
    "\n",
    "Производные функции потерь по параметрам $ U $ и $ V $ становятся более простыми благодаря свойствам нормы Фробениуса.\n",
    "\n",
    "### Вычисление градиентов\n",
    "\n",
    "Для нахождения градиентов по $ U $ и $ V $ мы применяем правила дифференцирования. Например, чтобы найти градиент по $ U $:\n",
    "\n",
    "1. Выражаем $ \\hat{A} = UV $ и находим производную:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial U} = -2 (A - UV) V^T\n",
    "$$\n",
    "\n",
    "2. Аналогично, для градиента по $ V $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial V} = -2 U^T (A - UV)\n",
    "$$\n",
    "\n",
    "Эти выражения являются очень удобными для вычисления, так как они легко обрабатываются с помощью матричных операций. Кроме того, использование нормы Фробениуса позволяет избежать сложных выражений, которые могут возникнуть при использовании других норм или метрик."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преимущества использования нормы Фробениуса\n",
    "\n",
    "1. **Простота**: Градиенты, полученные с использованием нормы Фробениуса, имеют простую и понятную форму, что облегчает реализацию алгоритмов оптимизации.\n",
    "\n",
    "2. **Эффективность**: Вычисление градиентов с использованием матричных операций часто оказывается более эффективным, особенно при работе с большими матрицами.\n",
    "\n",
    "3. **Сходимость**: Алгоритмы, использующие норму Фробениуса, часто демонстрируют хорошую сходимость и ведут к более стабильным результатам.\n",
    "\n",
    "Таким образом, использование нормы Фробениуса в задачах, связанных с матрицами, значительно упрощает процесс вычисления градиентов и, как следствие, оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2\n",
    "\n",
    "1. (1 pts) Compute the Jacobi matrix of the following function $ f : \\mathbb{R}^n \\to \\mathbb{R}^n $, defined as \n",
    "$\n",
    "f(w)_j = \\sum_{k=1}^{n} w_k e^{w_k}.\n",
    "$\n",
    "\n",
    "Also, consider how to compute this function in a stable manner if one of the elements in $ w $ is large. In this task, elementwise answer is Ok.\n",
    "\n",
    "2. (0.5 pts) Compute the gradient of the following functions with respect to matrix $ X $:\n",
    "\n",
    "    a) $ f(X) = \\sum_{i=1}^{n} \\lambda_i(X) $\n",
    "    \n",
    "    b) $ f(X) = \\prod_{i=1}^{n} \\lambda_i(X) $,\n",
    "    \n",
    "where $ \\lambda_i(X) $ is the $ i $-th eigenvalue of matrix $ X $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 1: Jacobi Matrix of Function $ f : \\mathbb{R}^n \\to \\mathbb{R}^n $\n",
    "\n",
    "The function $ f(w) $ is defined elementwise as:\n",
    "\n",
    "$$\n",
    "f(w)_j = \\sum_{k=1}^{n} w_k e^{w_k}.\n",
    "$$\n",
    "\n",
    "#### Jacobi Matrix\n",
    "\n",
    "The Jacobi matrix $ J_f(w) $ of a vector-valued function $ f: \\mathbb{R}^n \\to \\mathbb{R}^n $ is an $ n \\times n $ matrix where the entry at the $ j $-th row and $ k $-th column is given by the partial derivative $ \\frac{\\partial f_j}{\\partial w_k} $.\n",
    "\n",
    "For the given function, the partial derivatives are:\n",
    "\n",
    "- If $ j = k $:\n",
    "  $$\n",
    "  \\frac{\\partial f(w)_j}{\\partial w_k} = e^{w_k} + w_k e^{w_k}.\n",
    "  $$\n",
    "\n",
    "- If $ j \\neq k $:\n",
    "  $$\n",
    "  \\frac{\\partial f(w)_j}{\\partial w_k} = e^{w_k}.\n",
    "  $$\n",
    "\n",
    "Thus, the Jacobi matrix $ J_f(w) $ is:\n",
    "\n",
    "$$\n",
    "J_f(w) = \n",
    "\\begin{bmatrix}\n",
    "e^{w_1} + w_1 e^{w_1} & e^{w_2} & \\cdots & e^{w_n} \\\\\n",
    "e^{w_1} & e^{w_2} + w_2 e^{w_2} & \\cdots & e^{w_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "e^{w_1} & e^{w_2} & \\cdots & e^{w_n} + w_n e^{w_n}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "#### Stability Consideration\n",
    "\n",
    "If any component $ w_k $ is large, the expression $ w_k e^{w_k} $ can result in overflow. To compute $ w_k e^{w_k} $ more stably, consider using log transformations:\n",
    "\n",
    "$$\n",
    "w_k e^{w_k} = \\exp(\\log(w_k) + w_k)\n",
    "$$\n",
    "\n",
    "This transformation may help alleviate numerical instability with large $ w_k $.\n",
    "\n",
    "#### Part 2: Gradient with respect to Matrix $ X $\n",
    "\n",
    "##### Part 2a: $ f(X) = \\sum_{i=1}^{n} \\lambda_i(X) $\n",
    "\n",
    "For a symmetric matrix $ X $, the sum of its eigenvalues is equal to its trace:\n",
    "\n",
    "$$\n",
    "f(X) = \\sum_{i=1}^{n} \\lambda_i(X) = \\text{tr}(X).\n",
    "$$\n",
    "\n",
    "The gradient of the trace function with respect to $ X $ is simply the identity matrix:\n",
    "\n",
    "$$\n",
    "\\nabla_X f(X) = I.\n",
    "$$\n",
    "\n",
    "##### Part 2b: $ f(X) = \\prod_{i=1}^{n} \\lambda_i(X) $\n",
    "\n",
    "The product of eigenvalues of a matrix is its determinant:\n",
    "\n",
    "$$\n",
    "f(X) = \\det(X).\n",
    "$$\n",
    "\n",
    "The gradient of the determinant with respect to $ X $ is given by the cofactor matrix:\n",
    "\n",
    "$$\n",
    "\\nabla_X f(X) = \\det(X) \\cdot X^{-T}.\n",
    "$$\n",
    "\n",
    "For the gradient to exist and be well-defined, $ X $ should be invertible (i.e., full rank).\n",
    "\n",
    "These results rely on the properties of symmetric matrices and their eigenvalues. For non-symmetric matrices, one would have to consider more advanced techniques such as differentiating through the eigendecomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
