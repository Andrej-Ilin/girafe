{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pQWVWM3b85t"
   },
   "source": [
    "# Home assignment 3\n",
    "\n",
    "In this asignment, you will train your skills in implementation of the objective functions such that JAX can derive gradient function automatically without additional efforts from your side.\n",
    "We will start from the simple tasks and finish with more or less real scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSThpj4-ciux"
   },
   "source": [
    "## Task 1 (5 pts)\n",
    "\n",
    "The classical regression task aims to find such optinal parameters $w^*$ that minimizes the mean squared error loss functon\n",
    "\n",
    " $$ MSE = \\frac{1}{m} \\sum_{i=1}^m (y_i - f(w|x_i))^2, $$\n",
    "\n",
    " where $f(w|x_i)$ is a parametric function that maps input vector $x_i$ to a scalar $\\widehat{y}_i \\approx y_i$.\n",
    " Thus, we want to find such parameter $w^*$ that approximation of the ground-truth labels $y_i$ becomes as accurate as possible.\n",
    "\n",
    " Consider a particular instance of this problem, where\n",
    "\n",
    " $$ f(w|x) = \\cos(w_1x_1 + w_2) + \\exp(-w_5 x_2)\\sin(w_3x_2 + w_4)$$\n",
    "\n",
    " Write Python code that\n",
    " - (2 pts) implements two functons: $f$ and MSE\n",
    " - (1 pts) supports autograd from JAX\n",
    " - (2 pts) the resulting gradient function is fast and correct, i.e. the resulting gradient w.r.t. $w$ equals to the analytical gradient\n",
    "\n",
    " Demonstrate this on some demo input $(x_i, y_i)$ and random $w$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "aRw0Qmw3b6Y1"
   },
   "outputs": [],
   "source": [
    "# Your solution is here\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the parametric function f\n",
    "def f(w, x):\n",
    "    return jnp.cos(w[0] * x[0] + w[1]) + jnp.exp(-w[4] * x[1]) * jnp.sin(w[2] * x[1] + w[3])\n",
    "\n",
    "# Implements the Mean Squared Error function\n",
    "def mse(w, x, y):\n",
    "    return jnp.mean((y - f(w, x))**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate demo input data (x_i, y_i)\n",
    "X = jnp.array(np.random.randn(100, 2))  # 100 samples with 2 features\n",
    "y = jnp.array(np.random.randn(100, 1))   # 100 target values\n",
    "w = jnp.array([0.5, 0.5, 0.5, 0.5, 0.5])  # Initial weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function \\( f(w, x) \\) is defined as:\n",
    "$\n",
    "f(w, x) = \\cos(w[0] \\cdot x[0] + w[1]) + e^{-w[4] \\cdot x[1]} \\cdot \\sin(w[2] \\cdot x[1] + w[3])\n",
    "$\n",
    "\n",
    "Letâ€™s break down the gradient calculations for each weight:\n",
    "\n",
    "1. **Gradient with respect to \\( w[0] \\)**:\n",
    "   $\n",
    "   \\frac{\\partial f}{\\partial w[0]} = -\\sin(w[0] \\cdot x[0] + w[1]) \\cdot x[0]\n",
    "   $\n",
    "\n",
    "2. **Gradient with respect to \\( w[1] \\)**:\n",
    "   $\n",
    "   \\frac{\\partial f}{\\partial w[1]} = -\\sin(w[0] \\cdot x[0] + w[1])\n",
    "   $\n",
    "\n",
    "3. **Gradient with respect to \\( w[2] \\)**:\n",
    "   $\n",
    "   \\frac{\\partial f}{\\partial w[2]} = e^{-w[4] \\cdot x[1]} \\cdot x[1] \\cdot \\cos(w[2] \\cdot x[1] + w[3])\n",
    "   $\n",
    "\n",
    "4. **Gradient with respect to \\( w[3] \\)**:\n",
    "   $\n",
    "   \\frac{\\partial f}{\\partial w[3]} = e^{-w[4] \\cdot x[1]} \\cdot \\cos(w[2] \\cdot x[1] + w[3])\n",
    "   $\n",
    "\n",
    "5. **Gradient with respect to \\( w[4] \\)**:\n",
    "   $\n",
    "   \\frac{\\partial f}{\\partial w[4]} = -e^{-w[4] \\cdot x[1]} \\cdot x[1] \\cdot \\sin(w[2] \\cdot x[1] + w[3])\n",
    "   $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical gradient function\n",
    "def analytical_gradient(w, x):\n",
    "    df_dw0 = -jnp.sin(w[0] * x[:, 0] + w[1]) * x[:, 0]\n",
    "    df_dw1 = -jnp.sin(w[0] * x[:, 0] + w[1])\n",
    "    df_dw2 = jnp.exp(-w[4] * x[:, 1]) * x[:, 1] * jnp.cos(w[2] * x[:, 1] + w[3])\n",
    "    df_dw3 = jnp.exp(-w[4] * x[:, 1]) * jnp.cos(w[2] * x[:, 1] + w[3])\n",
    "    df_dw4 = -jnp.exp(-w[4] * x[:, 1]) * x[:, 1] * jnp.sin(w[2] * x[:, 1] + w[3])\n",
    "    \n",
    "    return jnp.array([jnp.mean(df_dw0), jnp.mean(df_dw1), jnp.mean(df_dw2), jnp.mean(df_dw3), jnp.mean(df_dw4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 2.4340803623199463\n",
      "MSE Gradient (automatic) = [-0.14500625 -0.6360937  -0.71605384  1.7825898  -0.76763594]\n",
      "\n",
      "Analytical Gradient = [-0.4106308  -0.36518627 -0.37647083  0.89268416 -0.22020797]\n"
     ]
    }
   ],
   "source": [
    "# Compute Mean Squared Error\n",
    "mse_val = mse(w, X, y)\n",
    "print(f'MSE = {mse_val}')\n",
    "\n",
    "# Compute the gradient of the MSE with respect to w\n",
    "mse_grad = grad(mse)(w, X, y)\n",
    "print(f'MSE Gradient (automatic) = {mse_grad}')\n",
    "print()\n",
    "# Compute analytical gradient\n",
    "grad_f = analytical_gradient(w, X)\n",
    "print(f'Analytical Gradient = {grad_f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are close: False\n"
     ]
    }
   ],
   "source": [
    "# Check if both gradients are close\n",
    "comparison = jnp.allclose(mse_grad, grad_f)\n",
    "print(f'Gradients are close: {comparison}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKFDFFqNuS0o"
   },
   "source": [
    "## Task 2 (8 pts)\n",
    "\n",
    "The classical multiclass classification problem (with $C$ classes) aims to minimize so-called CrossEntropy loss function\n",
    "\n",
    "$$xEntropy = -\\frac{1}{N} \\sum_{i=1}^N \\log \\left( \\frac{\\exp(\\widehat{y}_{iy_i})}{\\sum_{j=1}^C \\exp(\\widehat{y}_{ij})}\\right),$$\n",
    "\n",
    "where $\\widehat{y}_i$ is the vector of shape $C$ which stores estimation of non-normalized probabilities corresponding to possible classes to assign vector $x_i$.\n",
    "For example, if $C = 3$, the ground-truth class label $y_i = 0$ and $\\widehat{y}_i = [1, 10, -3]$, then $x_i$ more likely belongs to class 1, than to classes 0 and 3 although $\\widehat{y}_{iy_i} = 1$.\n",
    "\n",
    "So, given data matrix $X \\in \\mathbb{R}^{N \\times n}$ and the corresponding ground-truth class labels $y$ of shape $N$ and $y_i \\in \\{ 0, \\ldots C-1 \\}$.\n",
    "To estimate $\\widehat{y}_i$, one can construct arbitrary function depending on the set of parameters.\n",
    "In this task, let us approximate it through simple linear model which trasforms the input vector $x_i$ of shape $n$ as $\\widehat{y}_i = Wx_i + b$.\n",
    "So, the parameters of the model are $(W, b)$.\n",
    "\n",
    "- (3 pts) Implement crossentropy function in a stable manner\n",
    "- (2 pts) Implement function (which also called a model) that generates samples' embeddings $\\widehat{y}_i$ via linear model. Note that linear model $Wx + b$ is the simplest option, so you can compose this function from linear and non-linear functions. Pay attention that it depends on the data matrix, parameters $(W, b)$ for every linear function (and more parameters for nonlinear functions if any)\n",
    "- (2 pts) Generate with JAX the function to compute gradient of crossentropy function w.r.t. parameters of the model and check that it works and sufficiently fast.\n",
    "- (1 pts) Check numerically that your function works correcly. Numerical check means that your gradient function gives similar result to naive finite difference approximation of the gradient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Gradient W:\n",
      " [[ 0.1656944  -0.16569445]\n",
      " [ 0.02136332 -0.02136338]]\n",
      "Numerical Gradient W:\n",
      " [[-0.02980232 -0.02980232]\n",
      " [ 0.          0.        ]]\n",
      "Analytical Gradient b:\n",
      " [-0.14433108  0.14433107]\n",
      "Numerical Gradient b:\n",
      " [-0.08940697  0.17881393]\n"
     ]
    }
   ],
   "source": [
    "# Cross-entropy loss function\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    N = y_true.shape[0]\n",
    "    max_logits = jnp.max(y_pred, axis=1, keepdims=True)\n",
    "    log_probs = y_pred - max_logits - jnp.log(jnp.sum(jnp.exp(y_pred - max_logits), axis=1, keepdims=True))\n",
    "    true_log_probs = jnp.take_along_axis(log_probs, y_true[:, None], axis=1).flatten()\n",
    "    xEntropy = -jnp.mean(true_log_probs)\n",
    "    return xEntropy\n",
    "\n",
    "# Linear model function\n",
    "def linear_model(X, W, b):\n",
    "    return jnp.dot(X, W) + b\n",
    "\n",
    "# Combined loss and gradient function\n",
    "def loss_and_grads(W, b, X, y):\n",
    "    y_pred = linear_model(X, W, b)\n",
    "    loss = cross_entropy(y, y_pred)\n",
    "    return loss\n",
    "\n",
    "# Gradient functions\n",
    "grad_W = grad(loss_and_grads, argnums=0)\n",
    "grad_b = grad(loss_and_grads, argnums=1)\n",
    "\n",
    "# Numerical gradient function\n",
    "def numerical_gradient(func, params, epsilon=1e-6):\n",
    "    grads = jnp.zeros_like(params)\n",
    "    for i in range(params.size):\n",
    "        params_plus = params.at[i].set(params[i] + epsilon)\n",
    "        params_minus = params.at[i].set(params[i] - epsilon)\n",
    "        \n",
    "        # Compute the finite difference\n",
    "        grad = (func(params_plus) - func(params_minus)) / (2 * epsilon)\n",
    "        grads = grads.at[i].set(grad)\n",
    "    return grads\n",
    "\n",
    "# Example usage\n",
    "W = jnp.array([[0.1, 0.2], [0.3, 0.4]])  # Example weight matrix\n",
    "b = jnp.array([0.1, 0.2])  # Example bias vector\n",
    "X = jnp.array([[1.0, 2.0], [3.0, 4.0]])  # Example input data\n",
    "y = jnp.array([0, 1])  # Example ground truth labels\n",
    "\n",
    "# Compute analytical gradients\n",
    "analytical_grad_W = grad_W(W, b, X, y)\n",
    "analytical_grad_b = grad_b(W, b, X, y)\n",
    "\n",
    "# Function to compute loss with both W and b\n",
    "def loss_with_params(params):\n",
    "    W, b = params\n",
    "    return loss_and_grads(W, b, X, y)\n",
    "\n",
    "# Compute numerical gradients\n",
    "numerical_grad_W = numerical_gradient(lambda W: loss_with_params((W, b)), W)\n",
    "numerical_grad_b = numerical_gradient(lambda b: loss_with_params((W, b)), b)\n",
    "\n",
    "# Check if they are close\n",
    "print(\"Analytical Gradient W:\\n\", analytical_grad_W)\n",
    "print(\"Numerical Gradient W:\\n\", numerical_grad_W)\n",
    "print(\"Analytical Gradient b:\\n\", analytical_grad_b)\n",
    "print(\"Numerical Gradient b:\\n\", numerical_grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0, 1], dtype=int32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = jnp.array([0, 1])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc8ecVNq4EEq"
   },
   "source": [
    "## Task 3 (6 pts)\n",
    "\n",
    "The classical recommender system model is based on the regularized matrix factorization task, which aims to approximate the given matrix with user ratings to items by product of two smaller matrices $U, V$ as follows\n",
    "\n",
    "$$ L(U, V) = \\|X - UV \\|_F^2 + \\frac{\\lambda}{2} \\|U\\|_F^2 + \\frac{\\lambda}{2} \\|V\\|_F^2, $$\n",
    "\n",
    "where $X \\in \\mathbb{R}^{m \\times n}$, $U \\in \\mathbb{R}^{m \\times k}$ and $V \\in \\mathbb{R}^{k \\times n}$ and $k$ is much smaller $m$ and $n$.\n",
    "For example, if $10^5$ users rate $10^6$ items, then $m = 10^5$ and $n = 10^6$.\n",
    "In addition, $\\lambda > 0$ is a given constant.\n",
    "\n",
    "- (2 pts) Implement function $L$ in JAX for given matrix $X$\n",
    "- (1 pts) Verify that JAX can compute gradient w.r.t. $U$ and $V$. Use $m = 1000, n = 100, k = 10$ to generate synthetic matrices, and $\\lambda = 1$.\n",
    "- (3 pts) Compare the runtime for autograd and analytical expressions for the gradient for range of $m$, $n$ and $k$. What approach (analytical or autograd) is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "MS7QVxtK4Eaq"
   },
   "outputs": [],
   "source": [
    "def loss(U, V, X, lambda_):\n",
    "    frobenius_norm = jnp.linalg.norm(X - jnp.dot(U, V), ord='fro')**2\n",
    "    regularization = (lambda_ / 2) * (jnp.linalg.norm(U, ord='fro')**2 + jnp.linalg.norm(V, ord='fro')**2)\n",
    "    return frobenius_norm + regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Value: 1079256.0\n",
      "Gradient with respect to U shape: (1000, 10)\n",
      "Gradient with respect to V shape: (10, 100)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "m, n, k = 1000, 100, 10\n",
    "lambda_ = 1.0\n",
    "\n",
    "# Generate synthetic data\n",
    "key = random.PRNGKey(0)\n",
    "X = random.normal(key, (m, n))\n",
    "U = random.normal(key, (m, k))\n",
    "V = random.normal(key, (k, n))\n",
    "\n",
    "# Compute loss\n",
    "loss_value = loss(U, V, X, lambda_)\n",
    "\n",
    "# Compute gradients\n",
    "grad_U = grad(loss, argnums=0)(U, V, X, lambda_)\n",
    "grad_V = grad(loss, argnums=1)(U, V, X, lambda_)\n",
    "\n",
    "# Output results\n",
    "print(\"Loss Value:\", loss_value)\n",
    "print(\"Gradient with respect to U shape:\", grad_U.shape)\n",
    "print(\"Gradient with respect to V shape:\", grad_V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 100, n: 10, k: 5 | Autograd Time: 0.047501s | Analytical Time: 0.000854s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 500, n: 50, k: 10 | Autograd Time: 0.036570s | Analytical Time: 0.005101s\n",
      "m: 1000, n: 100, k: 20 | Autograd Time: 0.045977s | Analytical Time: 0.006143s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def analytical_grad(U, V, X, lambda_):\n",
    "    \"\"\"\n",
    "    Compute the analytical gradients with respect to U and V.\n",
    "    \n",
    "    Args:\n",
    "        U: User feature matrix of shape (m, k).\n",
    "        V: Item feature matrix of shape (k, n).\n",
    "        X: Rating matrix of shape (m, n).\n",
    "        lambda_: Regularization parameter.\n",
    "        \n",
    "    Returns:\n",
    "        grad_U, grad_V: The computed gradients.\n",
    "    \"\"\"\n",
    "    UV = jnp.dot(U, V)\n",
    "    error = X - UV\n",
    "    grad_U = -2 * jnp.dot(error, V.T) + lambda_ * U\n",
    "    grad_V = -2 * jnp.dot(U.T, error) + lambda_ * V\n",
    "    return grad_U, grad_V\n",
    "\n",
    "# Function to compare runtimes\n",
    "# Function to compare runtimes\n",
    "def compare_runtimes(m_values, n_values, k_values):\n",
    "    for m, n, k in zip(m_values, n_values, k_values):\n",
    "        # Generate synthetic data\n",
    "        key = random.PRNGKey(0)\n",
    "        X = random.normal(key, (m, n))\n",
    "        U = random.normal(key, (m, k))\n",
    "        V = random.normal(key, (k, n))\n",
    "\n",
    "        # Measure autograd time\n",
    "        start_time = time.time()\n",
    "        _ = grad(loss, argnums=0)(U, V, X, lambda_)\n",
    "        _ = grad(loss, argnums=1)(U, V, X, lambda_)\n",
    "        autograd_time = time.time() - start_time\n",
    "\n",
    "        # Measure analytical time\n",
    "        start_time = time.time()\n",
    "        analytical_grad(U, V, X, lambda_)\n",
    "        analytical_time = time.time() - start_time\n",
    "\n",
    "        print(f\"m: {m}, n: {n}, k: {k} | Autograd Time: {autograd_time:.6f}s | Analytical Time: {analytical_time:.6f}s\")\n",
    "\n",
    "# Example values for m, n, k\n",
    "m_values = [100, 500, 1000]\n",
    "n_values = [10, 50, 100]\n",
    "k_values = [5, 10, 20]\n",
    "\n",
    "compare_runtimes(m_values, n_values, k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
